
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                  ARGS                   ‚îÇ PROFILE  ‚îÇ     USER      ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ                                         ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 22:30 -05 ‚îÇ 04 Nov 25 22:32 -05 ‚îÇ
‚îÇ start   ‚îÇ --driver=docker --cpus=2 --memory=4000m ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:21 -05 ‚îÇ 04 Nov 25 23:22 -05 ‚îÇ
‚îÇ image   ‚îÇ build -t devopshint/node-app:latest .   ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:22 -05 ‚îÇ 04 Nov 25 23:23 -05 ‚îÇ
‚îÇ service ‚îÇ nodejs-app --url                        ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:23 -05 ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ list                                    ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:29 -05 ‚îÇ 04 Nov 25 23:29 -05 ‚îÇ
‚îÇ service ‚îÇ nodejs-app -n default --url             ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:29 -05 ‚îÇ                     ‚îÇ
‚îÇ service ‚îÇ list                                    ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:30 -05 ‚îÇ 04 Nov 25 23:30 -05 ‚îÇ
‚îÇ service ‚îÇ nodejs-app -n default --url             ‚îÇ minikube ‚îÇ administrador ‚îÇ v1.37.0 ‚îÇ 04 Nov 25 23:30 -05 ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/11/04 23:21:50
Running on machine: jorgesanchez
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1104 23:21:50.212594    6806 out.go:360] Setting OutFile to fd 1 ...
I1104 23:21:50.213108    6806 out.go:413] isatty.IsTerminal(1) = true
I1104 23:21:50.213111    6806 out.go:374] Setting ErrFile to fd 2...
I1104 23:21:50.213114    6806 out.go:413] isatty.IsTerminal(2) = true
I1104 23:21:50.213240    6806 root.go:338] Updating PATH: /home/administrador/.minikube/bin
W1104 23:21:50.213709    6806 root.go:314] Error reading config file at /home/administrador/.minikube/config/config.json: open /home/administrador/.minikube/config/config.json: no such file or directory
I1104 23:21:50.214658    6806 out.go:368] Setting JSON to false
I1104 23:21:50.216097    6806 start.go:130] hostinfo: {"hostname":"jorgesanchez","uptime":1444,"bootTime":1762315066,"procs":76,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.6.87.2-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"guest","hostId":"942a5202-17f9-47b8-b8d9-abe061a1db7b"}
I1104 23:21:50.216908    6806 start.go:140] virtualization: kvm guest
I1104 23:21:50.218275    6806 out.go:179] üòÑ  minikube v1.37.0 on Ubuntu 24.04 (kvm/amd64)
I1104 23:21:50.220698    6806 notify.go:220] Checking for updates...
I1104 23:21:50.221104    6806 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1104 23:21:50.222444    6806 driver.go:421] Setting default libvirt URI to qemu:///system
I1104 23:21:50.368562    6806 docker.go:123] docker version: linux-28.5.1:Docker Engine - Community
I1104 23:21:50.368704    6806 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1104 23:21:50.566147    6806 info.go:266] docker info: {ID:7ef1896f-a0ef-4ccc-b583-2f2bdee33fed Containers:5 ContainersRunning:5 ContainersPaused:0 ContainersStopped:0 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:65 OomKillDisable:false NGoroutines:77 SystemTime:2025-11-04 23:21:50.554693213 -0500 -05 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8130854912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jorgesanchez Labels:[] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:b98a3aace656320842a23f4a392a33f46af97866 Expected:} RuncCommit:{ID:v1.3.0-0-g4ca628d1 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2]] Warnings:<nil>}}
I1104 23:21:50.566314    6806 docker.go:318] overlay module found
I1104 23:21:50.568817    6806 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1104 23:21:50.570297    6806 start.go:304] selected driver: docker
I1104 23:21:50.570308    6806 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 23:21:50.570353    6806 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1104 23:21:50.571418    6806 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1104 23:21:50.647594    6806 info.go:266] docker info: {ID:7ef1896f-a0ef-4ccc-b583-2f2bdee33fed Containers:5 ContainersRunning:5 ContainersPaused:0 ContainersStopped:0 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:65 OomKillDisable:false NGoroutines:77 SystemTime:2025-11-04 23:21:50.639435695 -0500 -05 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8130854912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jorgesanchez Labels:[] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:b98a3aace656320842a23f4a392a33f46af97866 Expected:} RuncCommit:{ID:v1.3.0-0-g4ca628d1 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2]] Warnings:<nil>}}
W1104 23:21:50.647818    6806 out.go:285] ‚ùó  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I1104 23:21:50.647865    6806 cni.go:84] Creating CNI manager for ""
I1104 23:21:50.647901    6806 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1104 23:21:50.647926    6806 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 23:21:50.650415    6806 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1104 23:21:50.651620    6806 cache.go:123] Beginning downloading kic base image for docker with docker
I1104 23:21:50.652819    6806 out.go:179] üöú  Pulling base image v0.0.48 ...
I1104 23:21:50.653946    6806 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1104 23:21:50.653984    6806 preload.go:146] Found local preload: /home/administrador/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1104 23:21:50.653989    6806 cache.go:58] Caching tarball of preloaded images
I1104 23:21:50.654046    6806 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1104 23:21:50.654860    6806 preload.go:172] Found /home/administrador/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1104 23:21:50.654869    6806 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1104 23:21:50.654935    6806 profile.go:143] Saving config to /home/administrador/.minikube/profiles/minikube/config.json ...
I1104 23:21:50.712505    6806 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1104 23:21:50.712513    6806 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1104 23:21:50.712533    6806 cache.go:232] Successfully downloaded all kic artifacts
I1104 23:21:50.712551    6806 start.go:360] acquireMachinesLock for minikube: {Name:mk67e1dc854739249618bdef8cf8b980a89ecd39 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1104 23:21:50.712726    6806 start.go:364] duration metric: took 139.987¬µs to acquireMachinesLock for "minikube"
I1104 23:21:50.712740    6806 start.go:96] Skipping create...Using existing machine configuration
I1104 23:21:50.712743    6806 fix.go:54] fixHost starting: 
I1104 23:21:50.712886    6806 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1104 23:21:50.757462    6806 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1104 23:21:50.757480    6806 fix.go:138] unexpected machine state, will restart: <nil>
I1104 23:21:50.759010    6806 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I1104 23:21:50.759130    6806 cli_runner.go:164] Run: docker start minikube
I1104 23:21:51.047620    6806 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1104 23:21:51.094912    6806 kic.go:430] container "minikube" state is running.
I1104 23:21:51.095169    6806 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1104 23:21:51.136458    6806 profile.go:143] Saving config to /home/administrador/.minikube/profiles/minikube/config.json ...
I1104 23:21:51.136620    6806 machine.go:93] provisionDockerMachine start ...
I1104 23:21:51.136671    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:51.172979    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:51.173497    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:51.173509    6806 main.go:141] libmachine: About to run SSH command:
hostname
I1104 23:21:51.175269    6806 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:48910->127.0.0.1:32768: read: connection reset by peer
I1104 23:21:54.312061    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1104 23:21:54.312076    6806 ubuntu.go:182] provisioning hostname "minikube"
I1104 23:21:54.312130    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:54.349228    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:54.349365    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:54.349369    6806 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1104 23:21:54.506691    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1104 23:21:54.506736    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:54.545275    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:54.545405    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:54.545417    6806 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1104 23:21:54.678445    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1104 23:21:54.678473    6806 ubuntu.go:188] set auth options {CertDir:/home/administrador/.minikube CaCertPath:/home/administrador/.minikube/certs/ca.pem CaPrivateKeyPath:/home/administrador/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/administrador/.minikube/machines/server.pem ServerKeyPath:/home/administrador/.minikube/machines/server-key.pem ClientKeyPath:/home/administrador/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/administrador/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/administrador/.minikube}
I1104 23:21:54.678497    6806 ubuntu.go:190] setting up certificates
I1104 23:21:54.678507    6806 provision.go:84] configureAuth start
I1104 23:21:54.678569    6806 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1104 23:21:54.720394    6806 provision.go:143] copyHostCerts
I1104 23:21:54.720703    6806 exec_runner.go:144] found /home/administrador/.minikube/ca.pem, removing ...
I1104 23:21:54.720711    6806 exec_runner.go:203] rm: /home/administrador/.minikube/ca.pem
I1104 23:21:54.720790    6806 exec_runner.go:151] cp: /home/administrador/.minikube/certs/ca.pem --> /home/administrador/.minikube/ca.pem (1094 bytes)
I1104 23:21:54.721042    6806 exec_runner.go:144] found /home/administrador/.minikube/cert.pem, removing ...
I1104 23:21:54.721048    6806 exec_runner.go:203] rm: /home/administrador/.minikube/cert.pem
I1104 23:21:54.721079    6806 exec_runner.go:151] cp: /home/administrador/.minikube/certs/cert.pem --> /home/administrador/.minikube/cert.pem (1139 bytes)
I1104 23:21:54.721370    6806 exec_runner.go:144] found /home/administrador/.minikube/key.pem, removing ...
I1104 23:21:54.721372    6806 exec_runner.go:203] rm: /home/administrador/.minikube/key.pem
I1104 23:21:54.721390    6806 exec_runner.go:151] cp: /home/administrador/.minikube/certs/key.pem --> /home/administrador/.minikube/key.pem (1679 bytes)
I1104 23:21:54.721610    6806 provision.go:117] generating server cert: /home/administrador/.minikube/machines/server.pem ca-key=/home/administrador/.minikube/certs/ca.pem private-key=/home/administrador/.minikube/certs/ca-key.pem org=administrador.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1104 23:21:54.734516    6806 provision.go:177] copyRemoteCerts
I1104 23:21:54.734966    6806 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1104 23:21:54.735000    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:54.772355    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:54.868695    6806 ssh_runner.go:362] scp /home/administrador/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I1104 23:21:54.889076    6806 ssh_runner.go:362] scp /home/administrador/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1104 23:21:54.909231    6806 ssh_runner.go:362] scp /home/administrador/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1104 23:21:54.927736    6806 provision.go:87] duration metric: took 249.219985ms to configureAuth
I1104 23:21:54.927748    6806 ubuntu.go:206] setting minikube options for container-runtime
I1104 23:21:54.927866    6806 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1104 23:21:54.927897    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:54.965098    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:54.965236    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:54.965240    6806 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1104 23:21:55.101174    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1104 23:21:55.101185    6806 ubuntu.go:71] root file system type: overlay
I1104 23:21:55.101249    6806 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1104 23:21:55.101300    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.144685    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:55.144827    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:55.144863    6806 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1104 23:21:55.282688    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1104 23:21:55.282736    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.321037    6806 main.go:141] libmachine: Using SSH client type: native
I1104 23:21:55.321175    6806 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1104 23:21:55.321184    6806 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1104 23:21:55.452345    6806 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1104 23:21:55.452358    6806 machine.go:96] duration metric: took 4.315733043s to provisionDockerMachine
I1104 23:21:55.452368    6806 start.go:293] postStartSetup for "minikube" (driver="docker")
I1104 23:21:55.452374    6806 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1104 23:21:55.452425    6806 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1104 23:21:55.452457    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.489645    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:55.590108    6806 ssh_runner.go:195] Run: cat /etc/os-release
I1104 23:21:55.593245    6806 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1104 23:21:55.593258    6806 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1104 23:21:55.593261    6806 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1104 23:21:55.593265    6806 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1104 23:21:55.593271    6806 filesync.go:126] Scanning /home/administrador/.minikube/addons for local assets ...
I1104 23:21:55.593647    6806 filesync.go:126] Scanning /home/administrador/.minikube/files for local assets ...
I1104 23:21:55.593870    6806 start.go:296] duration metric: took 141.495221ms for postStartSetup
I1104 23:21:55.593946    6806 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1104 23:21:55.593975    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.633460    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:55.723964    6806 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1104 23:21:55.727847    6806 fix.go:56] duration metric: took 5.015099831s for fixHost
I1104 23:21:55.727863    6806 start.go:83] releasing machines lock for "minikube", held for 5.015127531s
I1104 23:21:55.727905    6806 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1104 23:21:55.768273    6806 ssh_runner.go:195] Run: cat /version.json
I1104 23:21:55.768302    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.768938    6806 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1104 23:21:55.768990    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:55.797685    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:55.798077    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:56.230767    6806 ssh_runner.go:195] Run: systemctl --version
I1104 23:21:56.237718    6806 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1104 23:21:56.241542    6806 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1104 23:21:56.257825    6806 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1104 23:21:56.257880    6806 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1104 23:21:56.264863    6806 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1104 23:21:56.264874    6806 start.go:495] detecting cgroup driver to use...
I1104 23:21:56.264894    6806 detect.go:190] detected "systemd" cgroup driver on host os
I1104 23:21:56.265252    6806 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1104 23:21:56.278325    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1104 23:21:56.286768    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1104 23:21:56.294687    6806 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1104 23:21:56.294733    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1104 23:21:56.302230    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1104 23:21:56.310510    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1104 23:21:56.320468    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1104 23:21:56.329451    6806 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1104 23:21:56.336943    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1104 23:21:56.344686    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1104 23:21:56.352538    6806 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1104 23:21:56.361072    6806 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1104 23:21:56.368368    6806 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1104 23:21:56.375618    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:56.433550    6806 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1104 23:21:56.509096    6806 start.go:495] detecting cgroup driver to use...
I1104 23:21:56.509126    6806 detect.go:190] detected "systemd" cgroup driver on host os
I1104 23:21:56.509169    6806 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1104 23:21:56.521650    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1104 23:21:56.531253    6806 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1104 23:21:56.561127    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1104 23:21:56.570051    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1104 23:21:56.580087    6806 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1104 23:21:56.592971    6806 ssh_runner.go:195] Run: which cri-dockerd
I1104 23:21:56.595682    6806 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1104 23:21:56.602469    6806 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1104 23:21:56.616947    6806 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1104 23:21:56.674900    6806 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1104 23:21:56.714183    6806 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1104 23:21:56.714296    6806 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1104 23:21:56.729549    6806 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1104 23:21:56.739269    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:56.795409    6806 ssh_runner.go:195] Run: sudo systemctl restart docker
I1104 23:21:57.131952    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1104 23:21:57.141123    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1104 23:21:57.149653    6806 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1104 23:21:57.159097    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1104 23:21:57.167783    6806 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1104 23:21:57.224884    6806 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1104 23:21:57.263592    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:57.318721    6806 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1104 23:21:57.372647    6806 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1104 23:21:57.381668    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:57.440769    6806 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1104 23:21:57.639243    6806 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1104 23:21:57.648257    6806 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1104 23:21:57.648381    6806 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1104 23:21:57.651366    6806 start.go:563] Will wait 60s for crictl version
I1104 23:21:57.651405    6806 ssh_runner.go:195] Run: which crictl
I1104 23:21:57.654266    6806 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1104 23:21:57.734616    6806 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1104 23:21:57.734672    6806 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1104 23:21:57.810866    6806 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1104 23:21:57.835950    6806 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1104 23:21:57.836118    6806 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1104 23:21:57.878671    6806 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1104 23:21:57.881765    6806 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1104 23:21:57.890557    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 23:21:57.932387    6806 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1104 23:21:57.932487    6806 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1104 23:21:57.932557    6806 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1104 23:21:57.949056    6806 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1104 23:21:57.949065    6806 docker.go:621] Images already preloaded, skipping extraction
I1104 23:21:57.949116    6806 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1104 23:21:57.964445    6806 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1104 23:21:57.964455    6806 cache_images.go:85] Images are preloaded, skipping loading
I1104 23:21:57.964460    6806 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1104 23:21:57.964555    6806 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1104 23:21:57.964596    6806 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1104 23:21:58.126208    6806 cni.go:84] Creating CNI manager for ""
I1104 23:21:58.126234    6806 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1104 23:21:58.126284    6806 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1104 23:21:58.126303    6806 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1104 23:21:58.126396    6806 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1104 23:21:58.126461    6806 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1104 23:21:58.135635    6806 binaries.go:44] Found k8s binaries, skipping transfer
I1104 23:21:58.135685    6806 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1104 23:21:58.142895    6806 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1104 23:21:58.156185    6806 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1104 23:21:58.169538    6806 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1104 23:21:58.184819    6806 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1104 23:21:58.187636    6806 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1104 23:21:58.196415    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:58.251248    6806 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1104 23:21:58.288739    6806 certs.go:68] Setting up /home/administrador/.minikube/profiles/minikube for IP: 192.168.49.2
I1104 23:21:58.288746    6806 certs.go:194] generating shared ca certs ...
I1104 23:21:58.288756    6806 certs.go:226] acquiring lock for ca certs: {Name:mk816c13353993396ac15fde6a6430b8d4973467 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 23:21:58.289160    6806 certs.go:235] skipping valid "minikubeCA" ca cert: /home/administrador/.minikube/ca.key
I1104 23:21:58.289459    6806 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/administrador/.minikube/proxy-client-ca.key
I1104 23:21:58.289464    6806 certs.go:256] generating profile certs ...
I1104 23:21:58.289692    6806 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/administrador/.minikube/profiles/minikube/client.key
I1104 23:21:58.290028    6806 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/administrador/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1104 23:21:58.290393    6806 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/administrador/.minikube/profiles/minikube/proxy-client.key
I1104 23:21:58.290465    6806 certs.go:484] found cert: /home/administrador/.minikube/certs/ca-key.pem (1675 bytes)
I1104 23:21:58.290479    6806 certs.go:484] found cert: /home/administrador/.minikube/certs/ca.pem (1094 bytes)
I1104 23:21:58.290491    6806 certs.go:484] found cert: /home/administrador/.minikube/certs/cert.pem (1139 bytes)
I1104 23:21:58.290500    6806 certs.go:484] found cert: /home/administrador/.minikube/certs/key.pem (1679 bytes)
I1104 23:21:58.292848    6806 ssh_runner.go:362] scp /home/administrador/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1104 23:21:58.312090    6806 ssh_runner.go:362] scp /home/administrador/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1104 23:21:58.333761    6806 ssh_runner.go:362] scp /home/administrador/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1104 23:21:58.353476    6806 ssh_runner.go:362] scp /home/administrador/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1104 23:21:58.373076    6806 ssh_runner.go:362] scp /home/administrador/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1104 23:21:58.393530    6806 ssh_runner.go:362] scp /home/administrador/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1104 23:21:58.413632    6806 ssh_runner.go:362] scp /home/administrador/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1104 23:21:58.435401    6806 ssh_runner.go:362] scp /home/administrador/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1104 23:21:58.456241    6806 ssh_runner.go:362] scp /home/administrador/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1104 23:21:58.476863    6806 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1104 23:21:58.492265    6806 ssh_runner.go:195] Run: openssl version
I1104 23:21:58.499886    6806 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1104 23:21:58.511223    6806 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1104 23:21:58.516339    6806 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov  5 03:32 /usr/share/ca-certificates/minikubeCA.pem
I1104 23:21:58.516403    6806 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1104 23:21:58.523652    6806 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1104 23:21:58.533501    6806 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1104 23:21:58.537342    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1104 23:21:58.543917    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1104 23:21:58.550674    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1104 23:21:58.556740    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1104 23:21:58.562908    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1104 23:21:58.569105    6806 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1104 23:21:58.575782    6806 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1104 23:21:58.575883    6806 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1104 23:21:58.594908    6806 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1104 23:21:58.602939    6806 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1104 23:21:58.602946    6806 kubeadm.go:589] restartPrimaryControlPlane start ...
I1104 23:21:58.602985    6806 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1104 23:21:58.610734    6806 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1104 23:21:58.610781    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 23:21:58.655283    6806 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32771"
I1104 23:21:58.662499    6806 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1104 23:21:58.670032    6806 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1104 23:21:58.670050    6806 kubeadm.go:593] duration metric: took 67.101398ms to restartPrimaryControlPlane
I1104 23:21:58.670055    6806 kubeadm.go:394] duration metric: took 94.282324ms to StartCluster
I1104 23:21:58.670071    6806 settings.go:142] acquiring lock: {Name:mk676475a27ecf2ea2d8c0e5f3a940298947655c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 23:21:58.670152    6806 settings.go:150] Updating kubeconfig:  /home/administrador/.kube/config
I1104 23:21:58.670480    6806 lock.go:35] WriteFile acquiring /home/administrador/.kube/config: {Name:mkaa3bf3366fd5317adc6bbac6d4d3b21e1f8354 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1104 23:21:58.670668    6806 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1104 23:21:58.670765    6806 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1104 23:21:58.670794    6806 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1104 23:21:58.670840    6806 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1104 23:21:58.670848    6806 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1104 23:21:58.670851    6806 addons.go:247] addon storage-provisioner should already be in state true
I1104 23:21:58.670866    6806 host.go:66] Checking if "minikube" exists ...
I1104 23:21:58.670962    6806 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1104 23:21:58.670990    6806 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1104 23:21:58.671093    6806 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1104 23:21:58.671133    6806 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1104 23:21:58.672679    6806 out.go:179] üîé  Verifying Kubernetes components...
I1104 23:21:58.675614    6806 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1104 23:21:58.707612    6806 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1104 23:21:58.707621    6806 addons.go:247] addon default-storageclass should already be in state true
I1104 23:21:58.707637    6806 host.go:66] Checking if "minikube" exists ...
I1104 23:21:58.707833    6806 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1104 23:21:58.710519    6806 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1104 23:21:58.715481    6806 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1104 23:21:58.715493    6806 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1104 23:21:58.715565    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:58.755322    6806 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1104 23:21:58.755331    6806 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1104 23:21:58.755376    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1104 23:21:58.755484    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:58.786754    6806 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1104 23:21:58.794690    6806 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/administrador/.minikube/machines/minikube/id_rsa Username:docker}
I1104 23:21:58.798064    6806 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1104 23:21:58.832615    6806 api_server.go:52] waiting for apiserver process to appear ...
I1104 23:21:58.832658    6806 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1104 23:21:58.856118    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1104 23:21:58.896173    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1104 23:21:59.001787    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.001824    6806 retry.go:31] will retry after 135.456394ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1104 23:21:59.003917    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.003931    6806 retry.go:31] will retry after 204.690743ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.138382    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1104 23:21:59.191264    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.191290    6806 retry.go:31] will retry after 193.093099ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.209529    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1104 23:21:59.260994    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.261011    6806 retry.go:31] will retry after 193.953812ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.333318    6806 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1104 23:21:59.385081    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1104 23:21:59.436384    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.436397    6806 retry.go:31] will retry after 713.790788ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.455719    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1104 23:21:59.515243    6806 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.515258    6806 retry.go:31] will retry after 759.925452ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1104 23:21:59.833542    6806 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1104 23:21:59.844545    6806 api_server.go:72] duration metric: took 1.173860384s to wait for apiserver process to appear ...
I1104 23:21:59.844558    6806 api_server.go:88] waiting for apiserver healthz status ...
I1104 23:21:59.844569    6806 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1104 23:22:00.151302    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1104 23:22:00.275945    6806 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1104 23:22:00.675122    6806 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1104 23:22:00.675139    6806 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1104 23:22:00.675174    6806 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1104 23:22:00.736595    6806 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1104 23:22:00.736609    6806 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/start-kubernetes-service-cidr-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1104 23:22:00.845207    6806 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1104 23:22:00.848786    6806 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1104 23:22:00.848797    6806 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1104 23:22:00.980823    6806 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I1104 23:22:00.981993    6806 addons.go:514] duration metric: took 2.311203755s for enable addons: enabled=[storage-provisioner default-storageclass]
I1104 23:22:01.345715    6806 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1104 23:22:01.352163    6806 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1104 23:22:01.352175    6806 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1104 23:22:01.844707    6806 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1104 23:22:01.847749    6806 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I1104 23:22:01.848718    6806 api_server.go:141] control plane version: v1.34.0
I1104 23:22:01.848731    6806 api_server.go:131] duration metric: took 2.004170129s to wait for apiserver health ...
I1104 23:22:01.848736    6806 system_pods.go:43] waiting for kube-system pods to appear ...
I1104 23:22:01.854491    6806 system_pods.go:59] 7 kube-system pods found
I1104 23:22:01.854509    6806 system_pods.go:61] "coredns-66bc5c9577-ll5ln" [2dd5f743-22ef-4708-a79a-0e1329e5613b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1104 23:22:01.854513    6806 system_pods.go:61] "etcd-minikube" [efe73ecd-31b5-490b-827e-f0bbd9e0d03e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1104 23:22:01.854518    6806 system_pods.go:61] "kube-apiserver-minikube" [eae88f3e-9fb9-4e74-bcb5-4deb2b7e808c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1104 23:22:01.854522    6806 system_pods.go:61] "kube-controller-manager-minikube" [d37af248-6173-40dc-9385-b3cf57cbf6c5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1104 23:22:01.854524    6806 system_pods.go:61] "kube-proxy-xthr6" [96bd4430-158d-4cea-9905-0ca0268eebdc] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1104 23:22:01.854526    6806 system_pods.go:61] "kube-scheduler-minikube" [755fe002-00aa-4587-bbfc-7717aab3382f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1104 23:22:01.854528    6806 system_pods.go:61] "storage-provisioner" [3694a17f-871f-402d-bf9c-f54c696baa90] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1104 23:22:01.854532    6806 system_pods.go:74] duration metric: took 5.793029ms to wait for pod list to return data ...
I1104 23:22:01.854538    6806 kubeadm.go:578] duration metric: took 3.183858947s to wait for: map[apiserver:true system_pods:true]
I1104 23:22:01.854546    6806 node_conditions.go:102] verifying NodePressure condition ...
I1104 23:22:01.857705    6806 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1104 23:22:01.857727    6806 node_conditions.go:123] node cpu capacity is 16
I1104 23:22:01.857735    6806 node_conditions.go:105] duration metric: took 3.186646ms to run NodePressure ...
I1104 23:22:01.857743    6806 start.go:241] waiting for startup goroutines ...
I1104 23:22:01.857747    6806 start.go:246] waiting for cluster config update ...
I1104 23:22:01.857754    6806 start.go:255] writing updated cluster config ...
I1104 23:22:01.858204    6806 ssh_runner.go:195] Run: rm -f paused
I1104 23:22:01.882176    6806 out.go:179] üí°  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I1104 23:22:01.885885    6806 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 05 04:21:56 minikube systemd[1]: Stopping Docker Application Container Engine...
Nov 05 04:21:56 minikube dockerd[220]: time="2025-11-05T04:21:56.804112691Z" level=info msg="Processing signal 'terminated'"
Nov 05 04:21:56 minikube dockerd[220]: time="2025-11-05T04:21:56.805378979Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Nov 05 04:21:56 minikube dockerd[220]: time="2025-11-05T04:21:56.805670256Z" level=info msg="Daemon shutdown complete"
Nov 05 04:21:56 minikube dockerd[220]: time="2025-11-05T04:21:56.805773832Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=moby
Nov 05 04:21:56 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 05 04:21:56 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 05 04:21:56 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.894268092Z" level=info msg="Starting up"
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.895166681Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.895236287Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.895254092Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.903298022Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.907180369Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 05 04:21:56 minikube dockerd[812]: time="2025-11-05T04:21:56.918495233Z" level=info msg="Loading containers: start."
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.069760103Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count b979d05bf8f947cd33e0c5cdfa1efb29f526eaf1b1f8094bc514dac11d7a59ae], retrying...."
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101435304Z" level=warning msg="error locating sandbox id 0fb0b0304f71bb0e75eeb4e74e5a76d32220d03381c35e07b462d9bbe337ffc9: sandbox 0fb0b0304f71bb0e75eeb4e74e5a76d32220d03381c35e07b462d9bbe337ffc9 not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101475858Z" level=warning msg="error locating sandbox id 83eb2aace475f2cdc842e8ac27ef71466bd60fead46738c4cae783c33fb5ee66: sandbox 83eb2aace475f2cdc842e8ac27ef71466bd60fead46738c4cae783c33fb5ee66 not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101484184Z" level=warning msg="error locating sandbox id 9c2bca69137a5df0e1f8802565afee7ac0b4a603caec393ebd33a45091ca6f6a: sandbox 9c2bca69137a5df0e1f8802565afee7ac0b4a603caec393ebd33a45091ca6f6a not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101494484Z" level=warning msg="error locating sandbox id cad21f387ab59b0714d34c7c25e3a57ec13a22cc969ae014f20a0245d399b241: sandbox cad21f387ab59b0714d34c7c25e3a57ec13a22cc969ae014f20a0245d399b241 not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101502173Z" level=warning msg="error locating sandbox id 27cb3304e0b6aedc4fec34ceb2cd6b3ec0c5a5d273e39dd6769323f79fb1b16e: sandbox 27cb3304e0b6aedc4fec34ceb2cd6b3ec0c5a5d273e39dd6769323f79fb1b16e not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101507502Z" level=warning msg="error locating sandbox id f76787d106d6495365f4e0426de5ad72b6ebaf5404fe894c2e706e66a8bbf19d: sandbox f76787d106d6495365f4e0426de5ad72b6ebaf5404fe894c2e706e66a8bbf19d not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101512736Z" level=warning msg="error locating sandbox id d1accd119e7c5383e4146af5839438b9d0c2d3b3e3cc33fbb5f348d4979baf3d: sandbox d1accd119e7c5383e4146af5839438b9d0c2d3b3e3cc33fbb5f348d4979baf3d not found"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.101672126Z" level=info msg="Loading containers: done."
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.110914889Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.111022249Z" level=info msg="Initializing buildkit"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.126378504Z" level=info msg="Completed buildkit initialization"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.129893008Z" level=info msg="Daemon has completed initialization"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.129965609Z" level=info msg="API listen on /var/run/docker.sock"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.129992597Z" level=info msg="API listen on /run/docker.sock"
Nov 05 04:21:57 minikube dockerd[812]: time="2025-11-05T04:21:57.130022659Z" level=info msg="API listen on [::]:2376"
Nov 05 04:21:57 minikube systemd[1]: Started Docker Application Container Engine.
Nov 05 04:21:57 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Start docker client with request timeout 0s"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Loaded network plugin cni"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Setting cgroupDriver systemd"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 05 04:21:57 minikube cri-dockerd[1128]: time="2025-11-05T04:21:57Z" level=info msg="Start cri-dockerd grpc backend"
Nov 05 04:21:57 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 05 04:21:58 minikube cri-dockerd[1128]: time="2025-11-05T04:21:58Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-ll5ln_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"eb81d6227af99ed1d3760cca17b7612273cf47ca68e24105cf4ec2be1621d14e\""
Nov 05 04:21:59 minikube cri-dockerd[1128]: time="2025-11-05T04:21:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b78f219bacf0e84a2483ede09728ab7e0ed5650a5db48b4a375fb20ed89f4ae2/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:21:59 minikube cri-dockerd[1128]: time="2025-11-05T04:21:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/346835866efd635fe51e756b0374b116d2423b23e95cb3495d34d68232da7d4a/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:21:59 minikube cri-dockerd[1128]: time="2025-11-05T04:21:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0fd89100bb3454c43b6db10aed2d0c6c59fc58c743496984c61868182a2b1f45/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:21:59 minikube cri-dockerd[1128]: time="2025-11-05T04:21:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5f0e3478ba11632733287ad91237b2bc3a8bce2aa69d5ce2138c526426a9bcb/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:22:00 minikube cri-dockerd[1128]: time="2025-11-05T04:22:00Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 05 04:22:02 minikube cri-dockerd[1128]: time="2025-11-05T04:22:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6b7dee7b29d91d55b75b6ee482af1426be67ca49b7507f005522c2b81bec1388/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:22:02 minikube cri-dockerd[1128]: time="2025-11-05T04:22:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/70dadcf462c0b395f83bdc69f3fbe1ae4578b9bdfc73dba6a13a32f851c9e549/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:22:02 minikube cri-dockerd[1128]: time="2025-11-05T04:22:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/34d68c774b1f0ec11e8c4d50e5ab42c749018bc1dfbc0ffc2a340d38f762ae57/resolv.conf as [nameserver 192.168.49.1 search hitronhub.home tigoune.com.co options ndots:0]"
Nov 05 04:22:15 minikube dockerd[812]: 2025/11/05 04:22:15 http2: server: error reading preface from client @: read unix /var/run/docker.sock->@: read: connection reset by peer
Nov 05 04:22:35 minikube dockerd[812]: time="2025-11-05T04:22:35.231208050Z" level=info msg="ignoring event" container=f1d3b5c5c2d5b19d6cd8a9ab3dcc617c45f5fb36f7165bf8dae86795c6098f20 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 05 04:30:12 minikube cri-dockerd[1128]: time="2025-11-05T04:30:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c6f74ee7afc98817eee2fed070b532e671a1e96da9943b567e9e7ed87fc51b0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local hitronhub.home tigoune.com.co options ndots:5]"
Nov 05 04:30:26 minikube cri-dockerd[1128]: time="2025-11-05T04:30:26Z" level=info msg="Pulling image devopshint/node-app:latest: e9889cb20e5c: Downloading [=====>                                             ]  87.23MB/824.2MB"
Nov 05 04:30:36 minikube cri-dockerd[1128]: time="2025-11-05T04:30:36Z" level=info msg="Pulling image devopshint/node-app:latest: e9889cb20e5c: Downloading [=====================================>             ]  620.5MB/824.2MB"
Nov 05 04:30:46 minikube cri-dockerd[1128]: time="2025-11-05T04:30:46Z" level=info msg="Pulling image devopshint/node-app:latest: e9889cb20e5c: Extracting [======================>                            ]    376MB/824.2MB"
Nov 05 04:30:50 minikube cri-dockerd[1128]: time="2025-11-05T04:30:50Z" level=info msg="Stop pulling image devopshint/node-app:latest: Status: Downloaded newer image for devopshint/node-app:latest"


==> container status <==
CONTAINER           IMAGE                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c933a76c55959       devopshint/node-app@sha256:07efd4763fa7b96ca6227e88a126e3e9622708dde42f9afef1d31e166eec2c65   17 seconds ago      Running             nodejs-app                0                   6c6f74ee7afc9       nodejs-app-78c7f4b66-vtsz6
5124d6a820552       6e38f40d628db                                                                                 8 minutes ago       Running             storage-provisioner       2                   34d68c774b1f0       storage-provisioner
45756f9003672       52546a367cc9e                                                                                 9 minutes ago       Running             coredns                   1                   6b7dee7b29d91       coredns-66bc5c9577-ll5ln
f1d3b5c5c2d5b       6e38f40d628db                                                                                 9 minutes ago       Exited              storage-provisioner       1                   34d68c774b1f0       storage-provisioner
a3888f7b79136       df0860106674d                                                                                 9 minutes ago       Running             kube-proxy                1                   70dadcf462c0b       kube-proxy-xthr6
ac8041f575174       90550c43ad2bc                                                                                 9 minutes ago       Running             kube-apiserver            1                   b78f219bacf0e       kube-apiserver-minikube
460d014e18c7c       46169d968e920                                                                                 9 minutes ago       Running             kube-scheduler            1                   e5f0e3478ba11       kube-scheduler-minikube
7d2c8b26d8484       a0af72f2ec6d6                                                                                 9 minutes ago       Running             kube-controller-manager   1                   0fd89100bb345       kube-controller-manager-minikube
1b046700fb29d       5f1f5298c888d                                                                                 9 minutes ago       Running             etcd                      1                   346835866efd6       etcd-minikube
8a9790f86790c       52546a367cc9e                                                                                 58 minutes ago      Exited              coredns                   0                   eb81d6227af99       coredns-66bc5c9577-ll5ln
517c731e69386       df0860106674d                                                                                 58 minutes ago      Exited              kube-proxy                0                   d4dfcd924c96c       kube-proxy-xthr6
986893ed436d4       46169d968e920                                                                                 58 minutes ago      Exited              kube-scheduler            0                   51ab68b20ce7b       kube-scheduler-minikube
b842cc346ee72       a0af72f2ec6d6                                                                                 58 minutes ago      Exited              kube-controller-manager   0                   4891843165b08       kube-controller-manager-minikube
db7c033507d58       5f1f5298c888d                                                                                 58 minutes ago      Exited              etcd                      0                   7aabb1dc1e9fa       etcd-minikube
9bd40200def67       90550c43ad2bc                                                                                 58 minutes ago      Exited              kube-apiserver            0                   a101957a10d5f       kube-apiserver-minikube


==> coredns [45756f900367] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:37038 - 21907 "HINFO IN 5521550501100249121.4044025964389615501. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.083490136s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [8a9790f86790] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:60310 - 36272 "HINFO IN 4905048240197925234.7631013824878953766. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.105946105s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_04T22_32_24_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 05 Nov 2025 03:32:20 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 05 Nov 2025 04:30:54 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 05 Nov 2025 04:27:47 +0000   Wed, 05 Nov 2025 03:32:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 05 Nov 2025 04:27:47 +0000   Wed, 05 Nov 2025 03:32:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 05 Nov 2025 04:27:47 +0000   Wed, 05 Nov 2025 03:32:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 05 Nov 2025 04:27:47 +0000   Wed, 05 Nov 2025 03:32:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7940288Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7940288Ki
  pods:               110
System Info:
  Machine ID:                 14b8b20ccf04490ba5bd11dbab7783b1
  System UUID:                14b8b20ccf04490ba5bd11dbab7783b1
  Boot ID:                    b57b26b3-9b45-4c06-8c49-3831a0e40488
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nodejs-app-78c7f4b66-vtsz6          0 (0%)        0 (0%)      0 (0%)           0 (0%)         56s
  kube-system                 coredns-66bc5c9577-ll5ln            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     58m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         58m
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         58m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         58m
  kube-system                 kube-proxy-xthr6                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         58m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 58m                  kube-proxy       
  Normal   Starting                 9m4s                 kube-proxy       
  Normal   NodeHasSufficientMemory  58m                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeAllocatableEnforced  58m                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure    58m                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     58m                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeNotReady             58m                  kubelet          Node minikube status is now: NodeNotReady
  Normal   Starting                 58m                  kubelet          Starting kubelet.
  Normal   RegisteredNode           58m                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeReady                58m                  kubelet          Node minikube status is now: NodeReady
  Normal   Starting                 9m9s                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  9m9s (x8 over 9m9s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    9m9s (x8 over 9m9s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     9m9s (x7 over 9m9s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  9m9s                 kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 9m7s                 kubelet          Node minikube has been rebooted, boot id: b57b26b3-9b45-4c06-8c49-3831a0e40488
  Normal   RegisteredNode           9m3s                 node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov 5 03:58] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.024251] PCI: Fatal: No config space access function found
[  +0.032702] PCI: System does not support PCI
[  +0.116945] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.743221] pulseaudio[302]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.115694] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003769] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001359] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001468] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001796] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.161774] systemd-journald[39]: File /var/log/journal/942a520217f947b8b8d9abe061a1db7b/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +8.266162] WSL (263) ERROR: CheckConnection: getaddrinfo() failed: -3
[  +5.004976] WSL (263) ERROR: CheckConnection: getaddrinfo() failed: -3
[Nov 5 04:00] WSL (263) ERROR: CheckConnection: getaddrinfo() failed: -5
[Nov 5 04:28] WSL (263) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [1b046700fb29] <==
{"level":"warn","ts":"2025-11-05T04:22:00.083096Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.083274Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55506","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.083299Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55488","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.087599Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55554","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.096442Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55582","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.105891Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.110384Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55614","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.113681Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.118310Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55678","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.122834Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55698","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.128405Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55700","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.133530Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55712","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.137616Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55722","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.141451Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55746","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.145668Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.149646Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55796","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.153591Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55798","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.157484Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55808","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.162885Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55842","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.167105Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.171270Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.176381Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55898","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.196229Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55916","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.200350Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55944","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.204072Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55954","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.208190Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55974","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.212041Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.216109Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55990","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.222420Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56024","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.226990Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56044","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.231428Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56048","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.235130Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56060","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.239453Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56080","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.243806Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56096","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.247430Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56100","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.251116Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56116","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.259125Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56136","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.263506Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56154","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.270902Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56168","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.274490Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56180","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.278655Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56204","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.282084Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56210","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.285948Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56232","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.291044Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56248","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.295712Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56254","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.305139Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56270","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.342399Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56296","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.346622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56324","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.350575Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56364","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.354193Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56378","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.357891Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56400","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.361476Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56422","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.370955Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56426","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.374633Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56454","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.378416Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56480","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.382426Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56494","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.416335Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56526","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.420577Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56540","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.424733Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56572","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T04:22:00.458745Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56576","server-name":"","error":"EOF"}


==> etcd [db7c033507d5] <==
{"level":"warn","ts":"2025-11-05T03:32:19.696739Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54410","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.702268Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54436","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.708487Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54456","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.715583Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54464","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.722425Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54494","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.730627Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54512","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.736896Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54538","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.745982Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54568","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.753096Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54576","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.760702Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54590","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.767125Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54604","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.775405Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54630","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.783117Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54638","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.790246Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54666","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.797815Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54696","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.806253Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54722","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.841702Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.849322Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54750","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.855573Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54766","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.868647Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54820","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.874976Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54836","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.881797Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.891897Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54880","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.904917Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54908","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.910720Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54910","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.916384Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54926","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.922029Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54936","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.938918Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:54984","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.945792Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55014","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.954040Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55018","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.959808Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55040","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.965504Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55068","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.971162Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55088","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.978154Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55108","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.984338Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55126","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:19.996809Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55142","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.030519Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55164","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.038120Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55176","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.045314Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55186","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.051938Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55190","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.058795Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55210","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.065390Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55216","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.077247Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55234","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.083435Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55246","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.089707Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.095303Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55258","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.101430Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55284","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.108358Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55306","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.148345Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55332","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.157046Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55356","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.165130Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55390","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-05T03:32:20.237250Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:55402","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-05T03:43:15.368124Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":618}
{"level":"info","ts":"2025-11-05T03:43:15.385946Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":618,"took":"16.72475ms","hash":1090154892,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-05T03:43:15.386150Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1090154892,"revision":618,"compact-revision":-1}
{"level":"info","ts":"2025-11-05T03:48:42.012147Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":858}
{"level":"info","ts":"2025-11-05T03:48:42.018222Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":858,"took":"4.606208ms","hash":2739188338,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":909312,"current-db-size-in-use":"909 kB"}
{"level":"info","ts":"2025-11-05T03:48:42.018317Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2739188338,"revision":858,"compact-revision":618}
{"level":"warn","ts":"2025-11-05T03:52:49.684339Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"452.875993ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" limit:10000 revision:1276 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-05T03:52:49.686795Z","caller":"traceutil/trace.go:172","msg":"trace[1693682085] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:1278; }","duration":"455.654941ms","start":"2025-11-05T03:52:49.231018Z","end":"2025-11-05T03:52:49.686673Z","steps":["trace[1693682085] 'get authentication metadata'  (duration: 452.359349ms)"],"step_count":1}


==> kernel <==
 04:31:07 up 32 min,  0 users,  load average: 0.37, 0.36, 0.25
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [9bd40200def6] <==
I1105 03:32:20.813597       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1105 03:32:20.813610       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1105 03:32:20.814565       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1105 03:32:20.819999       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1105 03:32:20.820066       1 policy_source.go:240] refreshing policies
E1105 03:32:20.871610       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1105 03:32:20.914830       1 controller.go:667] quota admission added evaluator for: namespaces
I1105 03:32:20.920698       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 03:32:20.920879       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1105 03:32:20.944828       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 03:32:20.944939       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1105 03:32:20.985968       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1105 03:32:21.723864       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1105 03:32:21.732186       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1105 03:32:21.732246       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1105 03:32:22.437008       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1105 03:32:22.490393       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1105 03:32:22.634257       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1105 03:32:22.649295       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1105 03:32:22.651267       1 controller.go:667] quota admission added evaluator for: endpoints
I1105 03:32:22.660313       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1105 03:32:22.744145       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1105 03:32:23.663115       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1105 03:32:23.682924       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1105 03:32:23.697407       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1105 03:32:31.337011       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1105 03:32:31.539130       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1105 03:32:31.737763       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 03:32:31.743688       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 03:33:32.338215       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:33:46.390100       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:34:58.682308       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:35:11.009477       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:36:22.988664       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:36:30.175931       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:37:46.714056       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:37:51.240650       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:38:58.706187       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:38:58.949788       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:40:07.177067       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:40:15.123127       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:41:19.384107       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:41:29.545202       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:42:35.697886       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:42:36.139337       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:43:16.434564       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 03:44:00.446309       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:44:03.171967       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:45:21.732346       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:45:41.566575       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:46:49.079931       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:46:50.342182       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:48:14.587457       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:48:18.148440       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:49:27.179703       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:49:41.919996       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:50:45.424867       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:51:14.611804       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:52:07.846919       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 03:52:51.156711       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [ac8041f57517] <==
I1105 04:22:00.670816       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1105 04:22:00.670870       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1105 04:22:00.670903       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1105 04:22:00.670909       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1105 04:22:00.670956       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1105 04:22:00.671496       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I1105 04:22:00.671958       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1105 04:22:00.672022       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1105 04:22:00.672225       1 controller.go:90] Starting OpenAPI V3 controller
I1105 04:22:00.672277       1 naming_controller.go:299] Starting NamingConditionController
I1105 04:22:00.672293       1 establishing_controller.go:81] Starting EstablishingController
I1105 04:22:00.672482       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1105 04:22:00.672516       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1105 04:22:00.672528       1 crd_finalizer.go:269] Starting CRDFinalizer
I1105 04:22:00.678057       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1105 04:22:00.678377       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1105 04:22:00.678420       1 controller.go:119] Starting legacy_token_tracking_controller
I1105 04:22:00.678438       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1105 04:22:00.682556       1 controller.go:142] Starting OpenAPI controller
I1105 04:22:00.683825       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1105 04:22:00.683876       1 policy_source.go:240] refreshing policies
I1105 04:22:00.686336       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1105 04:22:00.706492       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1105 04:22:00.769422       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1105 04:22:00.769485       1 cache.go:39] Caches are synced for LocalAvailability controller
I1105 04:22:00.769614       1 aggregator.go:171] initial CRD sync complete...
I1105 04:22:00.769632       1 autoregister_controller.go:144] Starting autoregister controller
I1105 04:22:00.769635       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1105 04:22:00.769638       1 cache.go:39] Caches are synced for autoregister controller
I1105 04:22:00.769696       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1105 04:22:00.769710       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1105 04:22:00.769987       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1105 04:22:00.770938       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1105 04:22:00.770975       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1105 04:22:00.770993       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1105 04:22:00.771031       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1105 04:22:00.772227       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1105 04:22:00.773519       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1105 04:22:00.774865       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
E1105 04:22:00.777199       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1105 04:22:00.778536       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1105 04:22:00.908693       1 controller.go:667] quota admission added evaluator for: endpoints
I1105 04:22:01.594950       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1105 04:22:01.674535       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1105 04:22:04.145853       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1105 04:22:04.397468       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1105 04:22:04.497287       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1105 04:23:07.999686       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:23:28.174481       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:24:37.497793       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:24:58.989142       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:25:45.121942       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:26:25.212339       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:27:16.244704       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:27:53.138472       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:28:36.710250       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:29:01.692963       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:30:00.735494       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1105 04:30:11.807656       1 alloc.go:328] "allocated clusterIPs" service="default/nodejs-app" clusterIPs={"IPv4":"10.111.68.59"}
I1105 04:30:11.871330       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [7d2c8b26d848] <==
I1105 04:22:03.893027       1 controllermanager.go:781] "Started controller" controller="volumeattributesclass-protection-controller"
I1105 04:22:03.893230       1 vac_protection_controller.go:206] "Starting VAC protection controller" logger="volumeattributesclass-protection-controller"
I1105 04:22:03.893254       1 shared_informer.go:349] "Waiting for caches to sync" controller="VAC protection"
I1105 04:22:03.992628       1 controllermanager.go:781] "Started controller" controller="validatingadmissionpolicy-status-controller"
I1105 04:22:03.992672       1 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I1105 04:22:03.997782       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1105 04:22:04.005026       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1105 04:22:04.005822       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1105 04:22:04.007395       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1105 04:22:04.009341       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1105 04:22:04.011359       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1105 04:22:04.015600       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1105 04:22:04.021352       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1105 04:22:04.021382       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1105 04:22:04.021389       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1105 04:22:04.026057       1 shared_informer.go:356] "Caches are synced" controller="job"
I1105 04:22:04.027287       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1105 04:22:04.029591       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1105 04:22:04.035890       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1105 04:22:04.038457       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1105 04:22:04.040556       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1105 04:22:04.043630       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1105 04:22:04.043740       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1105 04:22:04.043840       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1105 04:22:04.043937       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1105 04:22:04.044764       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1105 04:22:04.045884       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1105 04:22:04.047095       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1105 04:22:04.048281       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1105 04:22:04.050495       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1105 04:22:04.064696       1 shared_informer.go:356] "Caches are synced" controller="node"
I1105 04:22:04.064745       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1105 04:22:04.064769       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1105 04:22:04.064772       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1105 04:22:04.064775       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1105 04:22:04.068111       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1105 04:22:04.070362       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1105 04:22:04.072539       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1105 04:22:04.073716       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1105 04:22:04.073742       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1105 04:22:04.073725       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1105 04:22:04.078034       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1105 04:22:04.080341       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1105 04:22:04.092544       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1105 04:22:04.093749       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1105 04:22:04.093770       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1105 04:22:04.093784       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1105 04:22:04.093761       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1105 04:22:04.093790       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1105 04:22:04.093850       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1105 04:22:04.093927       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1105 04:22:04.093969       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1105 04:22:04.094005       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1105 04:22:04.095418       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1105 04:22:04.095814       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1105 04:22:04.098909       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1105 04:22:04.099195       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1105 04:22:04.099358       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1105 04:22:04.103731       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1105 04:22:04.108007       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [b842cc346ee7] <==
I1105 03:32:30.334566       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint_slice"
I1105 03:32:30.534398       1 controllermanager.go:781] "Started controller" controller="validatingadmissionpolicy-status-controller"
I1105 03:32:30.535521       1 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
I1105 03:32:30.561500       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1105 03:32:30.572633       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1105 03:32:30.575187       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1105 03:32:30.582443       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1105 03:32:30.583903       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1105 03:32:30.584046       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1105 03:32:30.584117       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1105 03:32:30.584166       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1105 03:32:30.586331       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1105 03:32:30.586949       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1105 03:32:30.587191       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1105 03:32:30.588088       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1105 03:32:30.588249       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1105 03:32:30.588463       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1105 03:32:30.588507       1 shared_informer.go:356] "Caches are synced" controller="job"
I1105 03:32:30.588796       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1105 03:32:30.589141       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1105 03:32:30.590505       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1105 03:32:30.593531       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1105 03:32:30.603816       1 shared_informer.go:356] "Caches are synced" controller="node"
I1105 03:32:30.603887       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1105 03:32:30.603910       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1105 03:32:30.603914       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1105 03:32:30.603918       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1105 03:32:30.612515       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1105 03:32:30.615018       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1105 03:32:30.626911       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1105 03:32:30.631628       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1105 03:32:30.631949       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1105 03:32:30.631996       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1105 03:32:30.632008       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1105 03:32:30.632252       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1105 03:32:30.632397       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1105 03:32:30.633133       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1105 03:32:30.633187       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1105 03:32:30.633477       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1105 03:32:30.633753       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1105 03:32:30.634319       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1105 03:32:30.634663       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1105 03:32:30.636301       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1105 03:32:30.636467       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1105 03:32:30.636543       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1105 03:32:30.636543       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1105 03:32:30.636589       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1105 03:32:30.637839       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1105 03:32:30.639315       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1105 03:32:30.639393       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1105 03:32:30.641979       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1105 03:32:30.644085       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1105 03:32:30.645396       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1105 03:32:30.651940       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1105 03:32:30.654871       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1105 03:32:30.662606       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1105 03:32:30.664148       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1105 03:32:30.666724       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1105 03:32:30.691064       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1105 03:32:40.587742       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"


==> kube-proxy [517c731e6938] <==
I1105 03:32:32.370830       1 server_linux.go:53] "Using iptables proxy"
I1105 03:32:32.514585       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1105 03:32:32.615376       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1105 03:32:32.615480       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1105 03:32:32.615704       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1105 03:32:32.644075       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1105 03:32:32.644177       1 server_linux.go:132] "Using iptables Proxier"
I1105 03:32:32.649119       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1105 03:32:32.655559       1 server.go:527] "Version info" version="v1.34.0"
I1105 03:32:32.655591       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 03:32:32.658214       1 config.go:106] "Starting endpoint slice config controller"
I1105 03:32:32.658249       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1105 03:32:32.658216       1 config.go:200] "Starting service config controller"
I1105 03:32:32.658267       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1105 03:32:32.658277       1 config.go:309] "Starting node config controller"
I1105 03:32:32.658304       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1105 03:32:32.658323       1 config.go:403] "Starting serviceCIDR config controller"
I1105 03:32:32.658327       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1105 03:32:32.758904       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1105 03:32:32.759009       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1105 03:32:32.759032       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1105 03:32:32.759050       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-proxy [a3888f7b7913] <==
I1105 04:22:02.363279       1 server_linux.go:53] "Using iptables proxy"
I1105 04:22:02.513856       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1105 04:22:02.615288       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1105 04:22:02.615321       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1105 04:22:02.615364       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1105 04:22:02.646614       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1105 04:22:02.646678       1 server_linux.go:132] "Using iptables Proxier"
I1105 04:22:02.650690       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1105 04:22:02.655529       1 server.go:527] "Version info" version="v1.34.0"
I1105 04:22:02.655584       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 04:22:02.656965       1 config.go:200] "Starting service config controller"
I1105 04:22:02.657007       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1105 04:22:02.657019       1 config.go:106] "Starting endpoint slice config controller"
I1105 04:22:02.657042       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1105 04:22:02.657075       1 config.go:403] "Starting serviceCIDR config controller"
I1105 04:22:02.657081       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1105 04:22:02.657341       1 config.go:309] "Starting node config controller"
I1105 04:22:02.657392       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1105 04:22:02.758140       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1105 04:22:02.758172       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1105 04:22:02.758160       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1105 04:22:02.758144       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [460d014e18c7] <==
I1105 04:21:59.810545       1 serving.go:386] Generated self-signed cert in-memory
W1105 04:22:00.686197       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1105 04:22:00.686266       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1105 04:22:00.686283       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1105 04:22:00.686291       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1105 04:22:00.739988       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1105 04:22:00.740022       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 04:22:00.741185       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1105 04:22:00.741641       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1105 04:22:00.741672       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1105 04:22:00.741646       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1105 04:22:00.842165       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [986893ed436d] <==
I1105 03:32:19.383414       1 serving.go:386] Generated self-signed cert in-memory
W1105 03:32:20.734047       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1105 03:32:20.734116       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1105 03:32:20.734130       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1105 03:32:20.734139       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1105 03:32:20.753220       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1105 03:32:20.753267       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1105 03:32:20.755327       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1105 03:32:20.755401       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1105 03:32:20.755412       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1105 03:32:20.755462       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1105 03:32:20.757584       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1105 03:32:20.758272       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1105 03:32:20.758580       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1105 03:32:20.758625       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1105 03:32:20.758797       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1105 03:32:20.759696       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1105 03:32:20.760004       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1105 03:32:20.760590       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1105 03:32:20.760694       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1105 03:32:20.760731       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1105 03:32:20.760772       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1105 03:32:20.760864       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1105 03:32:20.761043       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1105 03:32:20.760892       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1105 03:32:20.760945       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1105 03:32:20.760976       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1105 03:32:20.761060       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1105 03:32:20.761272       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1105 03:32:20.761146       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1105 03:32:21.566107       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1105 03:32:21.568594       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1105 03:32:21.605877       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1105 03:32:21.648000       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1105 03:32:21.648636       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1105 03:32:21.751967       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1105 03:32:21.781860       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1105 03:32:21.794240       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1105 03:32:21.819913       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1105 03:32:21.831306       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1105 03:32:21.831306       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1105 03:32:21.847415       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1105 03:32:21.881850       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1105 03:32:21.920186       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1105 03:32:21.963023       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1105 03:32:22.020348       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1105 03:32:22.098051       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1105 03:32:22.162817       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
I1105 03:32:25.256329       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.716935    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.716958    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.716997    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/dc6cf0a7bcb54d1f95cecc4d7b6b7d67-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"dc6cf0a7bcb54d1f95cecc4d7b6b7d67\") " pod="kube-system/kube-scheduler-minikube"
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.717096    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-certs\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.717160    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-data\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.717700    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.787673    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.789859    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.818695    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:58 minikube kubelet[1349]: I1105 04:21:58.898617    1349 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.899053    1349 kubelet_node_status.go:107] "Unable to register node with API server" err="Post \"https://192.168.49.2:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Nov 05 04:21:58 minikube kubelet[1349]: E1105 04:21:58.919825    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.020647    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.112356    1349 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://192.168.49.2:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.122181    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.222773    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: I1105 04:21:59.300401    1349 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.300785    1349 kubelet_node_status.go:107] "Unable to register node with API server" err="Post \"https://192.168.49.2:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.324067    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.333190    1349 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.424685    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.526294    1349 reconstruct.go:189] "Failed to get Node status to reconstruct device paths" err="Get \"https://192.168.49.2:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.606900    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.616722    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.627430    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:21:59 minikube kubelet[1349]: E1105 04:21:59.637862    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: I1105 04:22:00.102490    1349 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.648655    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.648655    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.648770    1349 kubelet.go:3215] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: I1105 04:22:00.789244    1349 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: I1105 04:22:00.789326    1349 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.789340    1349 kubelet_node_status.go:486] "Error updating node status, will retry" err="error getting node \"minikube\": node \"minikube\" not found"
Nov 05 04:22:00 minikube kubelet[1349]: I1105 04:22:00.790712    1349 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Nov 05 04:22:00 minikube kubelet[1349]: I1105 04:22:00.791290    1349 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.797640    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.898533    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:00 minikube kubelet[1349]: E1105 04:22:00.999645    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.100774    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.202075    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.302751    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.403974    1349 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.499035    1349 apiserver.go:52] "Watching apiserver"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.508955    1349 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.592903    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/96bd4430-158d-4cea-9905-0ca0268eebdc-xtables-lock\") pod \"kube-proxy-xthr6\" (UID: \"96bd4430-158d-4cea-9905-0ca0268eebdc\") " pod="kube-system/kube-proxy-xthr6"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.592940    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/96bd4430-158d-4cea-9905-0ca0268eebdc-lib-modules\") pod \"kube-proxy-xthr6\" (UID: \"96bd4430-158d-4cea-9905-0ca0268eebdc\") " pod="kube-system/kube-proxy-xthr6"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.592964    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/3694a17f-871f-402d-bf9c-f54c696baa90-tmp\") pod \"storage-provisioner\" (UID: \"3694a17f-871f-402d-bf9c-f54c696baa90\") " pod="kube-system/storage-provisioner"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.656613    1349 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.656703    1349 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Nov 05 04:22:01 minikube kubelet[1349]: I1105 04:22:01.656991    1349 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.666391    1349 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.666477    1349 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 05 04:22:01 minikube kubelet[1349]: E1105 04:22:01.666834    1349 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Nov 05 04:22:14 minikube kubelet[1349]: I1105 04:22:14.734747    1349 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Nov 05 04:22:35 minikube kubelet[1349]: I1105 04:22:35.825769    1349 scope.go:117] "RemoveContainer" containerID="7e4053b32ed4d5074860d698a5294a85c4b714b953c66c81e2b7fc0bd27a4e82"
Nov 05 04:22:35 minikube kubelet[1349]: I1105 04:22:35.825973    1349 scope.go:117] "RemoveContainer" containerID="f1d3b5c5c2d5b19d6cd8a9ab3dcc617c45f5fb36f7165bf8dae86795c6098f20"
Nov 05 04:22:35 minikube kubelet[1349]: E1105 04:22:35.826076    1349 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(3694a17f-871f-402d-bf9c-f54c696baa90)\"" pod="kube-system/storage-provisioner" podUID="3694a17f-871f-402d-bf9c-f54c696baa90"
Nov 05 04:22:49 minikube kubelet[1349]: I1105 04:22:49.431597    1349 scope.go:117] "RemoveContainer" containerID="f1d3b5c5c2d5b19d6cd8a9ab3dcc617c45f5fb36f7165bf8dae86795c6098f20"
Nov 05 04:30:11 minikube kubelet[1349]: I1105 04:30:11.843085    1349 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wd9cm\" (UniqueName: \"kubernetes.io/projected/73c72e6e-51a5-4fbf-837e-046e3b0205dc-kube-api-access-wd9cm\") pod \"nodejs-app-78c7f4b66-vtsz6\" (UID: \"73c72e6e-51a5-4fbf-837e-046e3b0205dc\") " pod="default/nodejs-app-78c7f4b66-vtsz6"
Nov 05 04:30:50 minikube kubelet[1349]: I1105 04:30:50.901574    1349 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nodejs-app-78c7f4b66-vtsz6" podStartSLOduration=4.725975401 podStartE2EDuration="39.901400145s" podCreationTimestamp="2025-11-05 04:30:11 +0000 UTC" firstStartedPulling="2025-11-05 04:30:12.319516086 +0000 UTC m=+452.953830645" lastFinishedPulling="2025-11-05 04:30:50.416110575 +0000 UTC m=+488.129255389" observedRunningTime="2025-11-05 04:30:50.899844323 +0000 UTC m=+488.612989135" watchObservedRunningTime="2025-11-05 04:30:50.901400145 +0000 UTC m=+488.614544955"


==> storage-provisioner [5124d6a82055] <==
W1105 04:30:01.827470       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:01.831577       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:03.833588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:03.837039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:05.842390       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:05.848421       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:07.851906       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:07.856538       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:09.858900       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:09.862166       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:11.865272       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:11.869457       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:13.874736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:13.879850       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:15.883016       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:15.890807       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:17.893738       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:17.897918       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:19.901056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:19.905969       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:21.909629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:21.914392       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:26.839641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:26.846500       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:28.851987       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:28.863043       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:30.866602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:30.872233       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:32.875640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:32.880678       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:34.884763       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:34.891835       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:36.894073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:36.902355       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:38.904929       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:38.909678       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:40.913052       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:40.916330       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:42.919624       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:42.924221       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:44.928129       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:44.933185       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:46.936773       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:46.943540       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:48.948175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:48.952056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:50.955128       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:50.959212       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:52.963735       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:52.970556       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:54.972928       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:54.977074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:56.980285       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:30:56.984818       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:01.932729       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:01.938877       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:03.944451       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:03.949764       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:05.954752       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1105 04:31:05.959296       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [f1d3b5c5c2d5] <==
I1105 04:22:02.272899       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1105 04:22:35.216464       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

